59 ㅇ
두번째로 역전파에서 신경써야할 부분은 기울기 소멸 입니다.

앞에서 퍼셉트론을 설명할 때, 층과 뉴런을 계속 추가하면 추가할 수록
복잡한 문제들을 해결할 수 있다고 배웠습니다.

그렇다고 층을 너무 많이 추가하면 신경망이 학습을 못할 가능성이 높습니다.
왜냐하면 기울기 소멸이 나타나기 때문입니다.

기울기 소멸이란 ~ 의미하며
기울기 소멸의 원인을 해결하기 위해서는 적절한 활성화 함수와 배치 정규화를 이용해야합니다.

60 ㅇ
기울기 소멸의 첫번째 원인 활성화 함수를 살펴보기 전에
신경망 학습 과정을 다시 한번 점검하면

가중치 업데이트가 되기 위해서는 -a 부분이 계속해서 바뀌면서 가중치(w') 값도 계속해서 바뀌어야 합니다.
바꿔말하면 -a 부분이 0이되버리면 가중치(w')의 값이 변하지 않아서 학습이 되지 않는다는 것입니다.
그러면 우리는 -a 부분에서 왜 0이 되는지에 대한 원인을 알아봐야합니다.

이 -a 부분에 관한 식을 다시 표현하면 이렇게 나타낼 수 있습니다.
역전파에서 봤던 식이죠?

이 식에서 우리가 주목해야할 부분은 두번째 곱해진 부분입니다.
두번째 곱해진 부분의 의미는 활성화 함수를 가중합에 관해 미분한 값
즉, 활성화 함수의 접선의 기울기를 의미합니다.

이해를 돕기 위해 앞서 설명했던 활성화 함수들을 예로 들어보겠습니다.

61 ㅇ
먼저 시그모이드 함수를 미분한 값의 범위는 0 ~ 0.25입니다.
즉, 미분을 한 번 할때마다 미분값이 1/4로 줄어듭니다.

<< 칠판 설명 >>

역전파 과정을 한 번하면 1/4, 두 번하면 1/16, 세 번하면 1/64로 줄어들기 때문에
많은 횟수를 반복하면 기울기가 0에 가까워집니다.
즉, -a 부분값이 0에 가까워지므로 가중치의 변화가 일어나지않아서 학습이 되지 않는 것입니다.

62 ㅇ
하이퍼볼릭 탄젠트 함수를 미분한 값의 범위는 0 ~ 1입니다.

시그모이드 함수보다는 미분값이 덜 줄어들기는 하지만
역전파 과정에서 많은 횟수를 반복하면 1보다 작은 값을 계속해서 곱해주므로
여전히 기울기가 0에 가까워집니다.

63 ㅇ
렐루 함수를 미분한 값의 범위는 양수 일때는 1이지만 음수 일때 0이 되므로
여전히 문제점이 존재합니다.

64 ㅇ
이를 보완하기 위해 렐루 변형 함수인 리키 렐루 함수는
입력값이 음수일 경우에는 0이 아니라 0.001과 같은 매우 작은 수를 반환하도록 되어있습니다.
즉, 기울기가 0에 가까워지지 않으므로 역전파가 계속 진행되도 학습이 될 수 있습니다.

따라서, 시그모이드, 하이퍼볼릭 탄젠트 함수보다는
렐루나 리키 렐루와 같은 렐루 함수의 변형을 사용하는 것이 좋다고 할 수 있습니다.

65 ㅇ
기울기 소멸을 해결하는 또 다른 방법은 배치 정규화를 이용하는 것입니다.
배치 정규화는 ~으로 하는 것입니다.

66 ㅇ
배치 정규화를 함으로써 ~ 할 수 있습니다.

67 ㅇ
마지막으로 역전파에서 신경써야 할 부분은 과적합 입니다.

과적합이란 ~ 이며
과적합을 예방하는 방법에는 드롭 아웃과 조기 종료가 있습니다.

68 ㅇ
먼저 드롭아웃은 ~ 입니다.

이렇게 노드들을 하나씩 비활성화하면 신경망은 노드들을 덜 학습하게 되겠죠?

69 ㅇ
조기종료는 ~입니다.

신경망 학습에 직접적으로 관여하지는 않고
개발자들이 의도적으로 중단시켜주는 방법입니다.




