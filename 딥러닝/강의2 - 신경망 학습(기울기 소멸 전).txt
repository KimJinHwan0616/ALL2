26 ㅇ
1장에서는 인공 신경망의 '정의'와 인공 신경망의 '종류'인 퍼셉트론, 심층 신경망과
관련된 용어들을 살펴보았습니다.

2장에서는 신경망이 어떻게 학습하는지에 대한 '학습 과정'과
학습 과정 중에 발생하는 여러 가지 '문제점'들을 알아보겠습니다.

27
2장 신경망 학습의 핵심은 ~이고 ~ 입니다.

이 핵심 내용들을 머릿속에 기억해두시고 세미나를 들으시면
개념과 원리를 쉽게 이해하는데 도움이 될 것입니다.

28 ㅇ
신경망 학습은 순전파와 역전파가 반복되는 과정을 의미하며
순전파는 입력층에서 출력층으로 가면서 실제값에서 예측값을 뺀 오차를 계산하고
역전파는 오차를 가지고 출력층에서 입력층으로 가중지＆편향을 업데이트 하는 과정입니다.

29 ㅇ
먼저 순전파에서 오차를 계산하기 위한 함수에는 손실 함수와 비용 함수가 있습니다.
손실 함수는 ~ 입니다.

30 ㅇ
비용 함수는 ~ 입니다.

흔히 loss(손실)과 cost(비용)라는 용어를 명확히 구분해서 사용하지는 않지만
공부하는 입장에서는 정확한 용어를 알아 둘 필요가 있으므로
각자 공부를 해두고 편하신 용어를 사용하시면 됩니다.

비용 함수에는 여러 종류가 함수가 있지만
대표적으로 평균 제곱 오차, 크로스 엔트로피 오차를 살펴보겠습니다.

31 ㅇ
먼저 평균 제곱 오차는 회귀에 사용되는 오차이며
예측값에서 실제값을 뺀 후 제곱을 한 후에 합한 값들을 출력층 노드 개수로 나눠줍니다.


32 ㅇ
크로스 엔트로피 오차는 분류에 사용되는 오차이며
0~1사이의 예측값에 자연로그를 취하고 실제값인 0 또는 1을 곱한 후 합한 값들에 -를 해줍니다.

평균 제곱 오차와 크로스 엔트로피 오차의 공식은 외울 필요가 없습니다.
지금부터 설명하는 수식들은 다 라이브러리에 코드 한 줄로 구현이 가능하기 때문에
이렇게 구하는 구나 하고 이해하시고 넘어가면 됩니다.

33 ㅇ
다음은 역전파입니다.
역전파에서 신경써야할 부분은 옵티마이저, 기울기 소멸, 과적합 크게 3가지 입니다.

34 ㅇ
첫번째로 옵티마이저 입니다.
옵티마이저란 ~ 입니다.

옵티마이저 중 경사하강법과 아담이 있는데
먼저 경사 하강법은 비용 함수의 접선의 기울기를 이용해
오차가 최소가되는 방향으로 가중치를 수정하는 방법입니다.

예를들어 MSE(평균 제곱 오차)에 대한 가중치와 오차의 관계를 그래프로 나타냈을 때
접선의 기울기를 통해 가중치를 계속 수정해서 오차가 최소가 되는 방향으로 가중치를 업데이트합니다.
이 때, 오차가 최소가 되는 지점을 전역 최솟값이라고 합니다.

경사 하강법에서 가중치를 업데이트 하기위한 식은 다음과 같습니다.
신경망은 이 식을 통해서 가중치를 수정하 오차를 최소화합니다.

지금부여터 이 식이 왜 나왔는지와 학습률이 무엇인지를 알아보겠습니다.

<< 칠판 설명 >>

35 ㅇ
따라서, 학습률은 ~을 의미하며
학습률이 너무 작으면 학습 시간이 오래 걸리고
반대로 학습률이 너무 크면 무질서하게 학습하게 되므로
너무 크지도 적지도 않게 적절한 학습률을 정해줘야 합니다.

41 ㅇ
실제로 신경망이 어떻게 학습을 하는지 학습 과정을 예제를 통해 한 번 알아보겠습니다.

51 ㅇ
~ 입니다.

지금부터 경사 하강법에 관련된 에포크, 배치 크기, 배치 수의 의미를 알아보겠습니다.

52
배치는 ~를 의미합니다.
즉, 전체 데이터를 배치들의 합으로 표현할 수 있습니다.

배치 크기는 ~ 이며, 배치 수는 ~ 입니다.
에포크는 ~ 입니다.

예를들어, 전체 데이터가 2000개이고 배치 크기를 200이라고 할 때
~ 설명 ~


53 ㅇ
따라서, 경사 하강법을 배치 크기를 기준으로 구분할 수 있는데,
배치 크기가 1이면 ~    배치 크기가 2이면 ~    배치 크기가 전체 데이터수면 ~ 라고 하며

일반적으로 신경망은 배치 크기가 2이상인 경사 하강법을 사용합니다.

54 ㅇ
미니배치 경사 하강법에서 기울기를 구하기 위해서는
미니배치에서 각각 오차를 구하고 그 값을 평균 해서 평균 오차를 구합니다.

55 ㅇ
그런데 MSE처럼 이차함수 그래프가 아닌
그림과 같은 함수인 경우 오차의 최솟값을 찾을 때 문제가 발생합니다.

경사 하강법으로 오차가 최소가 되는 지점을 찾아야 하는데
엉뚱한 지점에서 신경망이 학습을 중단해버리는 것입니다.
이 지점을 지역 최솟값이라고합니다.

56 ㅇ
이 문제점을 해결하기 위해 경사 하강법에서 더 발전된 아담을 사용합니다.

////
아담의 자세한 원리는 복잡해서
따로 정리해서 추가로 자세하게 알려드릴 계획이므로

지금은 경사 하강법의 문제점들을 보완한 가장 좋은 기법이라고 이해하고 넘어가시면 됩니다.
////

먼저 모멘텀은 진행 방향으로 더 진행해 보는 것을 유도하는 방법입니다.

모멘텀은 지역 최솟값을 멈추지 않고 지나칠 수 있지만
전역 최솟값에서도 추가로 이동하여 지나칠 수 있습니다.

57 ㅇ
아다그라드는 처음에는 큰 보폭으로 가다가 극솟값에 근접할 때
보폭을 줄여 정밀하게 최솟값에 접근하는 방법입니다.

58 ㅇ
아담은 모멘텀과 아다그라드를 기법입니다.
즉, 모멘텀으로 이전 기울기의 관성을 반영하고
아다그라드로 학습 진행 상황에 따라 학습률을 적절하게 조절합니다.

