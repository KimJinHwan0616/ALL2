70 ㅇ
2장에서 신경망이 어떻게 학습하는지에 대한 '학습 과정'과
학습 과정 중에 발생하는 여러 가지 '문제점'들을 알아보았습니다.

이제 3장에서는 합성곱 신경망(CNN)이 만들어진 '등장 배경'과 이와 관련된 '용어'들을 살펴보겠습니다.

71 ㅇ
3장 합성곱 신경망에서의 핵심은 ~ 이고 ~ 입니다.

이 핵심 내용을 머릿속에 기억해두시고 세미나를 들으시면
개념과 원리를 쉽게 이해하는데 도움이 될 것입니다.


72 ㅇ
위 손글씨 이미지를 다층 퍼셉트론으로 분류하기 위해서는
이미지를 1차원으로 변환하여 입력층으로 사용해야 합니다.

그런데, 변환된 결과가 사람이 보기에도 이게 원래 어떤 이미지였는지 알아보기가 어렵습니다.
이는 기계도 마찬가지 입니다.
위의 결과는 변환 전에 가지고 있던 공간적인 구조 정보가 유실된 상태 입니다.

공간적인 구조 정보라는 것은
거리가 가까운 어떤 픽셀들끼리는 어떤 연관이 있고, 어떤 픽셀들끼리는 값이 비슷한지를 말합니다.

결국 이미지의 공간적인 구조 정보를 보존하면서 학습할 수 있는 방법이 필요해졌고,
이를 해결하기 위해 사용하는 신경망이 바로 합성곱 신경망입니다.

다시 말하면, 합성곱 신경망은 이미지 처리에 탁월한 성능을 보이는 신경망 입니다.

73 ㅇ
합성곱 신경망은 합성곱층과 풀링층을 거치면서
입력 이미지의 공간적인 구조 정보를 가지고 있는 특성 벡터(feature vector)를 추출합니다.

74 ㅇ
그 후 추출된 특성 벡터들은 완전연결층을 거치면서 1차원 벡터로 변환되며,
마지막으로 출력층에서 활성화 함수인 소프트맥스(softmax) 함수를 사용하여 최종 결과가 출력됩니다.

75 ㅇ
이미지는 단순 1차원의 데이터가 아닌 높이, 너비, 채널의 값을 갖는 3차원 데이터입니다.
높이는 세로 픽셀 개수를 너비는 가로 픽셀 개수를 채널은 색 성분을 의미하며
이미지가 컬러이면 3 값을 가지고 흑백이면 1 값을 갖습니다.

76 ㅇ
합성곱은 커널(nxn 행렬)로 이미지를 처음부터 끝까지 겹치고 훑으면서
커널의 겹쳐지는 부분의 이미지와 커널의 원소의 값을 모두 곱해서
모두 더한 값을 출력하는 연산을 말합니다.

좀 더 쉽게 이해하기 위해 용어와 함께 예시로 설명드리겠습니다.

77 ~ 79 ㅇ
커널은 합성곱층과 풀링층의 가중치를 의미합니다.

앞에서 공부했던 신경망 학습 정과정을 다시 한번 기억해보면
신경망의 학습 과정은 최적의 가중치를 찾아가는 과정이라 했습니다.

그렇다면 합성곱 신경망에서 최적의 가중치를 찾아가는 과정은
최적의 가중치 값을 가지는 커널들을 찾는 과정이라고 생각할 수 있습니다.

다만, 커널의 크기는 우리가 직접 설정 해줘야한다는 점에서 약간 다릅니다.
일반적으로 3x3, 5x5 커널을 사용합니다.

특성 맵은 ~ 이며
컬러, 흑백 또는 커널 갯수에 따라서 특성 맵의 형태가 조금씩 달라집니다.

먼저 커널1개로 흑백 이미지에서 특성 맵(공간 정보)을 추출하는 과정을 알아보겠습니다.

- 입력 이미지와 필터를 포개 놓고 대응되는 숫자끼리 곱한 후 모두 더합니다.
- 필터가 1만큼 이동하여 같은 연산을 반복합니다.
- 연산이 끝나면 특성 맵이 추출됩니다.

80 ㅇ
커널 1개로 컬러 이미지에서 특성 맵(공간 정보)을 추출하는 과정은
앞서 다룬 흑백 이미지와 유사하지만
커널의 채널 수가 3이므로 RGB 각각에 서로 다른 가중치로 합성곱을 적용한 후
그 결과를 더해서 특성 맵(공간 정보)을 추출합니다.

81 ㅇ
여러 개 커널로 컬러 이미지에서 특성 맵(공간 정보)을 추출하는 과정에서
필터 각각은 특성 추출 결과의 채널이 됩니다.
각 특성 맵의 계산은 앞서 진행했던 방법과 동일합니다.

82 ㅇ
스트라이드는 ~ 의미합니다.
그림 처럼 스트라이드가 2라고 하면
5x5 입력에서 3x3 커널을 2칸씩 이동 시키므로
2x2 특성 맵이 만들어 집니다.

83 ㅇ
패딩은 입력 주위를 가상의 원소 0으로 채우는 것을 의미합니다.

<패딩 전 그림>처럼
패딩을 하지 않으면 합성곱의 결과로 얻은 특성 맵은 입력보다 크기가 무조건 작아질 수 밖에 없습니다.

하지만, <패딩 후 그림>처럼
패딩을 함으로써 합성곱 이후에도 특성 맵의 크기가 입력의 크기와 동일하게 유지할 수 있으므로
이미지의 크기를 유지하기 위해 패딩을 사용합니다.

84 ㅇ
위의 그림에서
패딩을 하지 않은 입력 데이터가 합성곱에 참여한 횟수를 구해보면
중앙이 4번 모서리가 1번 입니다.

패딩을 한 입력 데이터가 합성곱에 참여한 횟수를 구해보면
중앙이 9번 모서리가 4번 입니다.

다시 말하면, 패딩을 하지 않으면 중앙부와 모서리 픽셀이 합성곱에 참여하는 비율이 4:1로 크게 차이나지만
1픽셀을 패딩하면 이 차이는 9:4로 크게 줄어듭니다.

이처럼, 패딩은 합성곱에 입력 정보를 많이 참여시킴으로써 이미지 정보의 손실을 방지합니다.

85 ㅇ
풀링은 특성 맵의 가로, 세로 크기를 줄이는 연산이며 다운 샘플링이라고도 합니다.

풀링에는 최대 풀링과 평균 풀링이 있는데
먼저 최대 풀링은 커널 내 가장 큰 값을 선택하는 풀링입니다.

첫 번째 과정에서는 1,3,2,9 값 중 최댓값 9를 선택하고
두 번째 ~ 3번째 ~ 4번째 ~ㅂ

86 ㅇ
평균 풀링은 커널 내 평균값을 선택하는 풀링입니다.

첫 번째 과정에서는 1,3,2,9 값들을 평균한 값을 선택하고
두 번째 ~ 3번째 ~ 4번째 ~

-------------------------------

87 ㅇ
3장에서는 합성곱 신경망(CNN)이 만들어진 '등장 배경'과 이와 관련된 '용어'들을 살펴봤습니다.

4장에서는 사전 훈련된 모델을 사용해야 하는 '이유'와
합성곱 신경망(CNN)과 관련된 다양한 사전 훈련된 모델들의 '신경망 구조'를 알아보겠습니다.

88 ㅇ
4장 사전 훈련된 모델의 핵심은 ~ 이고 ~ 입니다.

이 핵심 내용들을 머릿속에 기억해두시고 세미나를 들으시면
개념과 원리를 쉽게 이해하는데 도움이 될 것입니다.

89 ㅇ
사전 훈련된 모델은 대량의 데이터셋을 미리 학습한 모델을 말합니다.

사전 훈련된 모델에서 가중치를 가져와서 학습하는 것을 전이 학습이라고 하며
전이 학습에는 어떤 층을 학습할 것인지에 따라 특성 추출과 미세 조정으로 구분할 수 있습니다.

90 ㅇ
특성 추출은 사전 훈련된 모델의 완전연결층만 학습합니다.
다시 말해서 합성곱층의 가중치를 고정시키고 완전연결층 가중치만 변화시키는 것입니다.

91 ㅇ
예를들어, 사전 훈련된 모델은 출력층에서 말, 쥐, 고양이, 개 등등 여러 가지 클래스로 분류하는 모델이지만
우리가 이진 분류 즉, 개와 고양이 클래스로만 분류를 원한다면
합성곱층에서의 공간 정보를 추출하는 학습 과정은 그대로 두고
완전연결층의 출력층 부분을 2개로 수정해서 개 또는 고양이만 분류하도록 학습시키는 것을 의미합니다.

92 ㅇ
미세 조정은 사전 훈련된 모델의 합성곱층 일부와 완전연결층을 학습하며 특성 추출보다 더 넓은 개념입니다.


