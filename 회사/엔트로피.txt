https://hyunw.kim/blog/2017/10/14/Entropy.html
1.
컴퓨터가 정보를 비교하기위해 정보를 비교할 수 있는 단위를 설정해줘야 했음
그래서 bit 단위로 수치화시킴 이것을 정보량이라고 하는데
이 것은 어떤 내용을 표현하기 위해 물어야 하는 '최소한의 질문개수'에서 출발함
(★ 정보량 = 질문개수)

예를들면,
전기 신호로 동전을 5번 던진 결과를 전송할 때,
10110(앞면-1, 뒷면-0)을 보낸다면 5개의 문자열을 보내기

ABCD 중 A를 구분하기 위해서 ABCD 중 앞쪽(AB)에 속하는지 뒷쪽(CD)에 속하는지를
판단하여 최대 2번만 구분하면 A를 추릴 수 있음

다시 말해서,
문자 2개를 구분하기 위한 질문개수는 1개
문자 4개를 구분하기 위한 질문개수는 2개
문자 8개를 구분하기 위한 질문개수는 3개
문자 n개를 구분하기 위한 질문개수는 2^n개

이를 수식화 하면
2^질문개수 = 문자의 개수

위의 예에서는 2^질문개수 = 4
우리가 알고 싶은건 '몇 번을 물어야 하는지' 이므로
질문개수 = 로그2(4)으로 나타낼 수 있음

이를 수식화 하면
질문개수 = log2(문자의 개수)

ㅁ 로그가 나타난 이유는 질문개수를 구분하려는 문자의 개수로 표현하기 위해 나온 것

## 일반화: 경우의 수(앞, 뒤)^질문개수 = 글자 총 개수
##############################################
2.
여기서 각각의 사건이 발생할 가능성 즉, '확률'이란 개념이 추가됩니다.

기계 X: 각각 동일한 확률(0.25) 출력(위의 예시 동일)
기계 Y: A - 0.5 / B - 0.125 / C - 0.125 / D- 0.25 확률 출력

기계 Y는 (A,B)중에 있니 (C,D)중에 있니라고 묻는 전략을 쓰는 것은 비효율적입니다
왜냐하면 이미 A가 50%의 확률로 나타나기 때문입니다.

그래서 처음에 물을 때 (A)니 아니면 (B,C,D)중에 있니 라고 묻는 게 옳습니다. ★

그리고 (B,C,D)중에서는 0.5의 확률 중 D가 0.25이므로
(D)니 아니면 (B,C) 중에 있니 라고 물으면 됩니다. ★

때문에, 기계 Y의 경우에 필요한 최소의 질문개수는
앞서 보였던 단순 로그식으로는 계산이 되지 않습니다.
(위의 식에서는 각각의 확률이 같다고 계산했기 때문!!)

그 대신 A 확률 X A의 질문개수(처음 1개의 질문만으로 A를 추려냅니다)
B 확률 X B의 질문개수(3번의 질문으로 추려냅니다)
C,D 동일

즉, p(A)⋅1 + p(B)⋅3 + p(C)⋅3 + p(D)⋅2 = 1.75
따라서, 1개의 글자를 기계 Y에서 구분하기 위해서는 평균적으로 1.75개의 질문이 필요

각 기계가 출력한 100개의 글자를 맞히기 위해서는
X는 200번, Y는 175번의 질문을 해야 합니다

따라서, Y가 X보다 더 적은 정보량을 생산한다고 볼 수 있습니다!

이를 식으로 정립한 것이 바로 섀먼이며
불확실성(H)의 측정을 엔트로피라고 부르고 단위를 bit라고 함
(모든 사건이 같은 확률로 일어나는 것이 가장 불확실(랜덤)합니다).

즉, 엔트로피는 불확실성(랜덤성)의 측정입니다.
##############################################
질문개수 = log2(문자의 개수)
p(A)⋅1 + p(B)⋅3 + p(C)⋅3 + p(D)⋅2 = 1.75
= p(A)⋅1 + p(B)⋅3 + p(C)⋅3 + p(D)⋅2 = 1.75

-> log2( 1 / 0.5 ) = 1, log2( 1 / 0.125 ) = 3   ## ???
(문자의 개수 = 1/사건 발생 확률 ★)

-> p(A)⋅log2( 1 / 1/2 ) + p(B)⋅log2( 1 / 1/8 ) + ...

일반화 하면
총 질문개수 = ∑(사건 발생 확률)⋅log2(1/사건 발생 확률)
= ∑Pi x log2(1 / Pi)= −∑사건 발생 확률 x log2(사건 발생 확률)

즉, 엔트로피 = 최적의 전략 하에서 그 사건을 예측하는 데 필요한 질문개수(H)

(정보량 = 질문개수)
엔트로피 감소 -> 사건을 맞추기 위한 질문개수 '감소' -> 사건 발생 확률 '증가' -> 정보량 '감소'
엔트로피 증가 -> 사건을 맞추기 위한 질문개수 '증가' -> 사건 발생 확률 '감소' -> 정보량 '증가'

(문자의 개수 = 1/사건 발생 확률 ★)
문자의 개수가 적다 -> 사건 발생 확률이 커진다 ->

<응용>
확률이 높은애는 가장 적은 비트를 할당하여 짧게 코딩
확률이 낮은애는 가장 많은 비트를 할당하여 길게 코딩
##############################################
크로스 엔트로피 오차: 어떤 문제(p)에 대해 특정 전략(q)을 쓸 때 예상되는 질문개수의 기댓값
즉, 어떤 문제 p에 대해 확률분포로 된 어떤 전략 q를 사용할 때의 질문개수의 기댓값

Pi : 특정 확률에 대한 참 값(실제값)
qi : 우리가 현재 학습한 확률값(예측값)

예를들면,
p = [0.5, 0.125, 0.125, 0.25]
q = [0.25, 0.25, 0.25, 0.25]

따라서, 우리가 어떤 qi 를 학습하고 있는 상태라면
pi에 가까워질수록 cross entropy 값은 작아지게 됩니다.

이런 특성 때문에 cross entropy 를 머신러닝에서 많이 쓰는 것입니다.
##############################################
<결론>
MSE는 어떤 값을 예측하는 것에 대한 오차이므로 실제값과 예측값들의 차이를 이용하여 계산하지만

분류는 각각의 클래스에 대한 확률을 예측하는 것이기 때문에
각각 클래스에 대한 확률의 실제값과 예측값들의 차이를 이용하여 계산해야함








